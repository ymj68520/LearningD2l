{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0945eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 导入必要的库 ====================\n",
    "# %matplotlib inline: 在Jupyter中内嵌显示matplotlib图像\n",
    "%matplotlib inline\n",
    "\n",
    "# torch: PyTorch深度学习框架\n",
    "import torch\n",
    "\n",
    "# torchvision: PyTorch的计算机视觉库，包含预训练模型和数据集\n",
    "import torchvision\n",
    "\n",
    "# nn: PyTorch的神经网络模块\n",
    "from torch import nn\n",
    "\n",
    "# F: PyTorch的函数式接口，包含各种操作函数\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# d2l: Dive into Deep Learning工具库\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 加载预训练的ResNet-18模型 ====================\n",
    "\n",
    "# 加载在ImageNet上预训练的ResNet-18模型\n",
    "# \n",
    "# 为什么使用预训练模型？\n",
    "# 1. 迁移学习：利用在大数据集上学到的特征\n",
    "# 2. 加速训练：不需要从头训练\n",
    "# 3. 更好的性能：预训练权重提供了良好的初始化\n",
    "pretrained_net = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# 查看ResNet-18的最后三层\n",
    "# children()返回模型的直接子模块列表\n",
    "# ResNet最后通常是：平均池化层、展平层、全连接层\n",
    "print(list(pretrained_net.children())[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 构建FCN的特征提取部分 ====================\n",
    "\n",
    "# 去掉ResNet-18的最后两层（全局平均池化和全连接层）\n",
    "# \n",
    "# 为什么要去掉？\n",
    "# 1. 全连接层会丢失空间信息\n",
    "# 2. 语义分割需要保留空间位置信息\n",
    "# 3. FCN的核心思想：全卷积网络，不使用全连接层\n",
    "# \n",
    "# [:-2] 保留除最后两层外的所有层\n",
    "# *list(...) 将列表展开为参数\n",
    "# nn.Sequential 将这些层组合成一个顺序模型\n",
    "net = nn.Sequential(*list(pretrained_net.children())[:-2])\n",
    "\n",
    "# 测试网络输出形状\n",
    "# 输入：1张图片，3通道(RGB)，320x480的分辨率\n",
    "X = torch.randn(1, 3, 320, 480)\n",
    "\n",
    "# 输出：特征图的形状\n",
    "# 经过ResNet卷积后，空间尺寸缩小，通道数增加到512\n",
    "# 预期输出形状：(1, 512, 10, 15)\n",
    "# 高度：320/32=10，宽度：480/32=15（ResNet下采样32倍）\n",
    "print(net(X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3cb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 添加FCN的分类和上采样层 ====================\n",
    "\n",
    "# Pascal VOC数据集包含21个类别\n",
    "# 0: 背景，1-20: 各种物体类别（飞机、自行车、鸟等）\n",
    "num_classes = 21\n",
    "\n",
    "# 添加1x1卷积层：将512个通道转换为21个类别通道\n",
    "# \n",
    "# 1x1卷积的作用：\n",
    "# 1. 降维：从512通道降到21通道\n",
    "# 2. 分类：每个通道对应一个类别的预测\n",
    "# 3. 保持空间信息：不改变特征图的高度和宽度\n",
    "net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
    "\n",
    "# 添加转置卷积层：将特征图上采样回原始图像大小\n",
    "# \n",
    "# 参数解释：\n",
    "# - in_channels=num_classes: 输入21个类别通道\n",
    "# - out_channels=num_classes: 输出21个类别通道\n",
    "# - kernel_size=64: 卷积核大小（较大的核用于平滑上采样）\n",
    "# - padding=16: 填充，用于调整输出尺寸\n",
    "# - stride=32: 步幅32，将特征图放大32倍（恢复到原始尺寸）\n",
    "# \n",
    "# 为什么stride=32？\n",
    "# 因为ResNet将图像下采样了32倍，现在需要上采样32倍恢复\n",
    "net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n",
    "                                               kernel_size=64, padding=16,\n",
    "                                               stride=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3edf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 双线性插值卷积核 ====================\n",
    "\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    \"\"\"\n",
    "    构造用于双线性插值的转置卷积核\n",
    "    \n",
    "    为什么需要双线性插值？\n",
    "    - 转置卷积的权重需要初始化\n",
    "    - 双线性插值是一种平滑的上采样方法\n",
    "    - 它比随机初始化的权重效果更好，训练更稳定\n",
    "    \n",
    "    双线性插值原理：\n",
    "    - 距离中心越近的像素权重越大\n",
    "    - 距离中心越远的像素权重越小\n",
    "    - 这样可以产生平滑的放大效果\n",
    "    \n",
    "    参数:\n",
    "        in_channels: 输入通道数\n",
    "        out_channels: 输出通道数\n",
    "        kernel_size: 卷积核大小\n",
    "    \n",
    "    返回:\n",
    "        weight: 初始化好的卷积核权重\n",
    "    \"\"\"\n",
    "    # 计算中心位置的因子\n",
    "    # factor用于计算权重衰减的速度\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    \n",
    "    # 确定卷积核的中心位置\n",
    "    if kernel_size % 2 == 1:\n",
    "        # 奇数大小：中心是整数位置\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        # 偶数大小：中心是0.5的位置\n",
    "        center = factor - 0.5\n",
    "    \n",
    "    # 创建网格坐标\n",
    "    # og[0]: 行坐标网格 (kernel_size, 1)\n",
    "    # og[1]: 列坐标网格 (1, kernel_size)\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "          torch.arange(kernel_size).reshape(1, -1))\n",
    "    \n",
    "    # 计算双线性插值的权重\n",
    "    # 公式：(1 - |x-center|/factor) * (1 - |y-center|/factor)\n",
    "    # 离中心越近，权重越大；离中心越远，权重越小\n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "           (1 - torch.abs(og[1] - center) / factor)\n",
    "    \n",
    "    # 初始化权重张量\n",
    "    # 形状：(in_channels, out_channels, kernel_size, kernel_size)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    \n",
    "    # 只为对角线位置（输入通道i对应输出通道i）赋值\n",
    "    # 这保证了每个通道独立处理，不会混合颜色通道\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 测试双线性插值上采样效果 ====================\n",
    "\n",
    "# 创建一个转置卷积层用于测试\n",
    "# 3个输入通道(RGB)，3个输出通道，卷积核4x4\n",
    "# padding=1, stride=2: 将图像放大2倍\n",
    "# bias=False: 不使用偏置项\n",
    "conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,\n",
    "                                bias=False)\n",
    "\n",
    "# 用双线性插值核初始化转置卷积的权重\n",
    "conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4))\n",
    "\n",
    "# 读取测试图像并转换为张量\n",
    "# ToTensor()会将PIL图像转换为形状(C, H, W)的张量，值域[0,1]\n",
    "img = torchvision.transforms.ToTensor()(d2l.Image.open('./catdog.jpg'))\n",
    "\n",
    "# 添加批次维度：(C, H, W) -> (1, C, H, W)\n",
    "X = img.unsqueeze(0)\n",
    "\n",
    "# 执行转置卷积，放大图像\n",
    "Y = conv_trans(X)\n",
    "\n",
    "# 移除批次维度并调整通道顺序以便显示\n",
    "# (1, C, H, W) -> (C, H, W) -> (H, W, C)\n",
    "out_img = Y[0].permute(1, 2, 0).detach()\n",
    "\n",
    "# 设置图像显示大小\n",
    "d2l.set_figsize()\n",
    "\n",
    "# 显示原始图像\n",
    "print('input image shape:', img.permute(1, 2, 0).shape)\n",
    "d2l.plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "# 显示放大后的图像（应该是原始图像的2倍大小）\n",
    "print('output image shape:', out_img.shape)\n",
    "d2l.plt.imshow(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 初始化FCN的转置卷积层 ====================\n",
    "\n",
    "# 使用双线性插值核初始化FCN网络中的转置卷积层\n",
    "# \n",
    "# 参数说明：\n",
    "# - num_classes: 21个类别，输入和输出通道都是21\n",
    "# - 64: 卷积核大小\n",
    "# \n",
    "# 这样初始化的好处：\n",
    "# 1. 提供了良好的起点，而不是随机权重\n",
    "# 2. 保证了平滑的上采样效果\n",
    "# 3. 加速训练收敛\n",
    "W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "net.transpose_conv.weight.data.copy_(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3256da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 加载VOC数据集 ====================\n",
    "\n",
    "# 设置批次大小和裁剪尺寸\n",
    "batch_size = 32           # 每批处理32张图像\n",
    "crop_size = (320, 480)    # 将图像裁剪为320x480\n",
    "\n",
    "# 加载VOC语义分割数据集\n",
    "# train_iter: 训练集数据迭代器\n",
    "# test_iter: 测试集数据迭代器\n",
    "# \n",
    "# 这个函数会：\n",
    "# 1. 下载VOC2012数据集（如果还没下载）\n",
    "# 2. 创建数据集对象\n",
    "# 3. 创建数据加载器，支持批处理和多进程加载\n",
    "train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44454a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 定义损失函数并开始训练 ====================\n",
    "\n",
    "def loss(inputs, targets):\n",
    "    \"\"\"\n",
    "    语义分割的损失函数\n",
    "    \n",
    "    参数:\n",
    "        inputs: 模型预测，形状(batch_size, num_classes, H, W)\n",
    "        targets: 真实标签，形状(batch_size, H, W)，每个元素是类别索引\n",
    "    \n",
    "    返回:\n",
    "        平均损失值\n",
    "    \n",
    "    实现细节：\n",
    "    - cross_entropy: 计算每个像素的交叉熵损失\n",
    "    - reduction='none': 不自动求平均，保留每个像素的损失\n",
    "    - .mean(1).mean(1): 对高度和宽度维度求平均\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n",
    "\n",
    "\n",
    "# ==================== 设置训练参数 ====================\n",
    "num_epochs = 5              # 训练5个epoch\n",
    "lr = 0.001                  # 学习率\n",
    "wd = 1e-3                   # 权重衰减（L2正则化）\n",
    "devices = d2l.try_all_gpus() # 尝试使用所有可用的GPU\n",
    "\n",
    "# 创建优化器\n",
    "# SGD: 随机梯度下降\n",
    "# weight_decay: 权重衰减，防止过拟合\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "# 开始训练\n",
    "# train_ch13是d2l提供的训练函数，专门用于计算机视觉任务\n",
    "# 它会：\n",
    "# 1. 在每个epoch遍历训练数据\n",
    "# 2. 计算损失并更新权重\n",
    "# 3. 在测试集上评估性能\n",
    "# 4. 绘制训练曲线\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 预测和可视化 ====================\n",
    "\n",
    "def predict(img):\n",
    "    \"\"\"\n",
    "    对单张图像进行语义分割预测\n",
    "    \n",
    "    参数:\n",
    "        img: 输入图像张量，形状(C, H, W)\n",
    "    \n",
    "    返回:\n",
    "        pred: 预测的类别索引，形状(H, W)\n",
    "    \"\"\"\n",
    "    # 归一化图像（使用与训练时相同的归一化）\n",
    "    # unsqueeze(0): 添加批次维度 (C, H, W) -> (1, C, H, W)\n",
    "    X = test_iter.dataset.normalize_image(img).unsqueeze(0)\n",
    "    \n",
    "    # 在GPU上进行预测\n",
    "    # net(X): 得到形状(1, 21, H, W)的输出，每个通道是一个类别的分数\n",
    "    # argmax(dim=1): 在类别维度上取最大值的索引，得到(1, H, W)\n",
    "    pred = net(X.to(devices[0])).argmax(dim=1)\n",
    "    \n",
    "    # 移除批次维度，返回(H, W)的类别索引\n",
    "    return pred.reshape(pred.shape[1], pred.shape[2])\n",
    "\n",
    "\n",
    "def label2image(pred):\n",
    "    \"\"\"\n",
    "    将类别索引转换为RGB彩色图像\n",
    "    \n",
    "    参数:\n",
    "        pred: 类别索引张量，形状(H, W)\n",
    "    \n",
    "    返回:\n",
    "        彩色分割图，形状(H, W, 3)\n",
    "    \"\"\"\n",
    "    # VOC_COLORMAP是预定义的颜色映射表\n",
    "    # 将其转换为张量并放到GPU上\n",
    "    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])\n",
    "    \n",
    "    # 确保pred是长整型（用作索引）\n",
    "    X = pred.long()\n",
    "    \n",
    "    # 使用类别索引从颜色映射表中查找对应的RGB颜色\n",
    "    # colormap[X, :] 会根据X中的类别索引返回对应的RGB值\n",
    "    return colormap[X, :]\n",
    "\n",
    "\n",
    "# ==================== 在测试集上进行预测和可视化 ====================\n",
    "\n",
    "# 下载VOC数据集并获取路径\n",
    "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\n",
    "\n",
    "# 读取测试集的图像和标注\n",
    "test_images, test_labels = d2l.read_voc_images(voc_dir, False)\n",
    "\n",
    "# 设置要显示的图像数量\n",
    "n, imgs = 4, []\n",
    "\n",
    "# 对前n张测试图像进行预测\n",
    "for i in range(n):\n",
    "    # 定义裁剪区域：从左上角(0,0)开始，裁剪320x480的区域\n",
    "    crop_rect = (0, 0, 320, 480)\n",
    "    \n",
    "    # 裁剪原始图像\n",
    "    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)\n",
    "    \n",
    "    # 预测并转换为彩色图像\n",
    "    pred = label2image(predict(X))\n",
    "    \n",
    "    # 收集三张图像：原图、预测结果、真实标注\n",
    "    imgs += [\n",
    "        X.permute(1, 2, 0),      # 原始图像，调整为(H,W,C)格式\n",
    "        pred.cpu(),               # 预测的彩色分割图，移到CPU\n",
    "        torchvision.transforms.functional.crop(\n",
    "            test_labels[i], *crop_rect).permute(1, 2, 0)  # 真实标注\n",
    "    ]\n",
    "\n",
    "# 显示结果：3行，每行n张图像\n",
    "# 第一行：原始图像\n",
    "# 第二行：模型预测结果\n",
    "# 第三行：真实标注\n",
    "# imgs[::3]: 每隔3个取一个，即原始图像\n",
    "# imgs[1::3]: 预测结果\n",
    "# imgs[2::3]: 真实标注\n",
    "d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
