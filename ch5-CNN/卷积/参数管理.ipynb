{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b741eec5",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "\n",
    "考虑在有module的情况下访问或者管理参数。\n",
    "\n",
    "先考虑一个单隐藏层MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d989782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1241],\n",
      "        [0.1389]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4,8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8,1)\n",
    ")\n",
    "X = torch.rand(size=(2,4))\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c45a2",
   "metadata": {},
   "source": [
    "## 获取一层中的n权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa08a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.1968,  0.2480,  0.3306,  0.1889, -0.2792,  0.0940,  0.2401,  0.1442]])), ('bias', tensor([0.0610]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())  # 访问第二层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e64dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0610], requires_grad=True)\n",
      "tensor([0.0610])\n"
     ]
    }
   ],
   "source": [
    "# 访问第二层的偏置参数\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a329c728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(net[2].weight.grad == None)  # 权重的梯度在反向传播前为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0352e335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# 一次性访问所有参数\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eecd869e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0610])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e1c078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1068],\n",
      "        [0.1068]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4,8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8,4),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(\n",
    "    block2(),\n",
    "    nn.Linear(4,1)\n",
    ")\n",
    "\n",
    "print(rgnet(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d27fd69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dad1ca",
   "metadata": {},
   "source": [
    "## 修改默认的初始参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdacb742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0051, -0.0083,  0.0073, -0.0136])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "net.apply(init_normal)\n",
    "print(net[0].weight.data[0])  # 查看第一层的权重参数\n",
    "print(net[0].bias.data)      # 查看第一层的偏置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8cb98ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "net.apply(init_constant)\n",
    "print(net[0].weight.data[0])  # 查看第一层的权重参数\n",
    "print(net[0].bias.data)      # 查看第一层的偏置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a73761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1444, -0.5032,  0.3079, -0.4662])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])  # 查看第一层的权重参数\n",
    "print(net[2].weight.data)      # 查看第二层的权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe68f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "Init ('weight', torch.Size([1, 8])) ('bias', torch.Size([1]))\n",
      "tensor([[ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [ 9.0355,  0.0000, -0.0000, -8.7955],\n",
      "        [ 9.7313, -8.7391,  5.5579,  0.0000],\n",
      "        [-5.8401,  0.0000, -9.5707, -0.0000],\n",
      "        [-9.2654,  6.4986, -8.7337,  5.2166],\n",
      "        [-5.4260, -8.8009,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "        \n",
    "net.apply(my_init)\n",
    "print(net[0].weight.data)  # 查看第一层的权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2c3ebc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42.,  2.,  2.,  2.])\n"
     ]
    }
   ],
   "source": [
    "# 简单 粗暴的方法\n",
    "net[0].weight.data[:] += 1.0\n",
    "net[0].weight.data[0, 0] = 42\n",
    "print(net[0].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aacd49",
   "metadata": {},
   "source": [
    "## 参数绑定\n",
    "\n",
    "e多个层共享一些参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14e3a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4,8),\n",
    "    nn.ReLU(),\n",
    "    shared,\n",
    "    nn.ReLU(),\n",
    "    shared,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8,1)\n",
    ")\n",
    "\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])  # True\n",
    "net[2].weight.data[0,0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])  # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829bff4",
   "metadata": {},
   "source": [
    "## 自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., -1.,  0.,  1.,  2.])\n"
     ]
    }
   ],
   "source": [
    "# 自定义一个无参数的层\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "    \n",
    "layer = CenteredLayer()\n",
    "print(layer(torch.FloatTensor([1,2,3,4,5])))  # tensor([-2., -1., 0., 1., 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0112cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.4506e-09, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(8,128),\n",
    "    CenteredLayer()\n",
    ")\n",
    "Y = net(torch.rand(4,8))\n",
    "print(Y.mean())  # tensor(-7.4506e-09, grad_fn=<MeanBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8f6e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.6810, -0.5173,  0.1590],\n",
      "        [ 1.8465,  1.6802,  0.7742],\n",
      "        [-0.1727,  1.8331, -1.3380],\n",
      "        [-0.6608,  0.1719, -0.6723],\n",
      "        [ 0.9717,  1.3464, -1.4139]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 自定义一个有参数的层\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features,))\n",
    "        # self.weight = nn.Parameter(torch.zeros(in_features, out_features))\n",
    "        # self.bias = nn.Parameter(torch.zeros(out_features,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "    \n",
    "dense = MyLinear(5,3)\n",
    "print(dense.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5783a7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7151, 1.4433, 0.0000],\n",
      "        [1.3851, 2.3780, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 使用自定义层i直接执行正向传播计算\n",
    "print(dense(torch.rand(2,5)))  # tensor([[0., 0.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98b27c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.8956],\n",
      "        [3.6193]])\n"
     ]
    }
   ],
   "source": [
    "# 使用自定义的层构建模型\n",
    "net = nn.Sequential(\n",
    "    MyLinear(64,8),\n",
    "    MyLinear(8,1)\n",
    ")\n",
    "\n",
    "print(net(torch.rand(2,64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622c46a",
   "metadata": {},
   "source": [
    "## 读写文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76f09a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "torch.save(x, 'x.pt')\n",
    "\n",
    "x2 = torch.load('x.pt', weights_only=True)\n",
    "print(x2)  # tensor([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02702ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x,y], 'x.pt')\n",
    "x2, y2 = torch.load('x.pt', weights_only=True)\n",
    "print(x2)  # tensor([0, 1, 2, 3])\n",
    "print(y2)  # tensor([0., 0., 0., 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43bde996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'xy.pt')\n",
    "mydict2 = torch.load('xy.pt', weights_only=True)\n",
    "print(mydict2['x'])  # tensor([0, 1, 2, 3])\n",
    "print(mydict2['y'])  # tensor([0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48aede49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载和保存模型参数\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.output = nn.Linear(256,10)\n",
    "    def forward(self, X):\n",
    "        return self.output(F.relu(self.hidden(X)))\n",
    "    \n",
    "net = MLP()\n",
    "X = torch.rand(size=(2,20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91c6a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc579a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params', weights_only=True))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8218c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "print(Y_clone == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386f7db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
