{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0589fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 导入必要的库 ====================\n",
    "# collections: 提供专门的容器数据类型，如Counter用于计数\n",
    "# re: 正则表达式模块，用于文本处理\n",
    "# d2l: Dive into Deep Learning工具库，提供深度学习相关的辅助函数\n",
    "import collections  # 导入collections模块，用于词频统计\n",
    "import re  # 导入正则表达式模块，用于文本清理\n",
    "from d2l import torch as d2l  # 从d2l导入torch相关工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2590fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n",
      "# 句子数: 3221\n",
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "# ==================== 下载和读取时间机器数据集 ====================\n",
    "# 什么是时间机器数据集？\n",
    "# - 《时间机器》小说全文\n",
    "# - 用于自然语言处理任务的经典数据集\n",
    "# - 包含大量英文文本，用于训练语言模型\n",
    "\n",
    "# 将数据集添加到d2l的数据中心\n",
    "# 'time_machine': 数据集名称\n",
    "# d2l.DATA_URL + 'timemachine.txt': 下载URL\n",
    "# '090b5e7e70c295757f55df93cb0a180b9691891a': 文件哈希值，用于验证完整性\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt', '090b5e7e70c295757f55df93cb0a180b9691891a')  # 注册数据集到d2l数据中心\n",
    "\n",
    "def read_time_machine():  # 定义读取时间机器数据集的函数\n",
    "    \"\"\"\n",
    "    加载时间机器数据集到文本行列表\n",
    "    \n",
    "    处理步骤：\n",
    "    1. 下载数据集文件\n",
    "    2. 读取所有行\n",
    "    3. 使用正则表达式清理文本：\n",
    "       - [^A-Za-z]+: 匹配非字母字符\n",
    "       - 替换为空格\n",
    "       - 转换为小写\n",
    "       - 去除首尾空格\n",
    "    \n",
    "    返回：\n",
    "        清理后的文本行列表\n",
    "    \"\"\"\n",
    "    with open(d2l.download('time_machine'), 'r') as f:  # 下载并打开文件\n",
    "        lines = f.readlines()  # 读取所有行到列表\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]  # 清理每一行：替换非字母为空格，转小写，去除首尾空格\n",
    "\n",
    "# 读取数据集\n",
    "lines = read_time_machine()  # 调用函数读取数据集\n",
    "\n",
    "# 输出基本信息\n",
    "print(f'# 句子数: {len(lines)}')  # 打印句子数量\n",
    "print(lines[0])  # 打印第一行\n",
    "print(lines[10])  # 打印第11行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b644efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "# ==================== 文本分词函数 ====================\n",
    "# 什么是分词（tokenization）？\n",
    "# - 将连续的文本分割成有意义的单元（词元）\n",
    "# - 词元可以是单词（word）或字符（char）\n",
    "# - 这是NLP预处理的基础步骤\n",
    "\n",
    "def tokenize(lines, token='word'):  # 定义分词函数，参数：文本行列表，分词类型（默认单词）\n",
    "    \"\"\"\n",
    "    将文本行拆分为单词或字符词元\n",
    "    \n",
    "    参数：\n",
    "        lines: 文本行列表\n",
    "        token: 分词类型，'word'（单词）或'char'（字符）\n",
    "    \n",
    "    返回：\n",
    "        分词后的词元列表，每个元素是一个词元列表\n",
    "    \n",
    "    示例：\n",
    "        输入：[\"hello world\", \"how are you\"]\n",
    "        输出（word）：[[\"hello\", \"world\"], [\"how\", \"are\", \"you\"]]\n",
    "        输出（char）：[[\"h\",\"e\",\"l\",\"l\",\"o\",\" \",\"w\",\"o\",\"r\",\"l\",\"d\"], ...]\n",
    "    \"\"\"\n",
    "    if token == 'word':  # 如果分词类型是单词\n",
    "        return [line.split() for line in lines]  # 按空格分割每一行，返回单词列表\n",
    "    elif token == 'char':  # 如果分词类型是字符\n",
    "        return [list(line) for line in lines]  # 将每一行转换为字符列表\n",
    "    else:  # 否则\n",
    "        print('错误：未知词元类型：' + token)  # 打印错误信息\n",
    "        \n",
    "# 对时间机器数据集进行分词（按单词）\n",
    "tokens = tokenize(lines)  # 调用分词函数，默认按单词分词\n",
    "\n",
    "# 显示前11行的分词结果\n",
    "for i in range(11):  # 循环前11次\n",
    "    print(tokens[i])  # 打印第i行的分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f54861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 词汇表（Vocab）类 ====================\n",
    "# 什么是词汇表？\n",
    "# - 将词元映射到数字索引的字典\n",
    "# - 用于将文本转换为模型可以处理的数字序列\n",
    "# - 通常按词频排序，最常见的词有较小的索引\n",
    "\n",
    "class Vocab:  # 定义词汇表类\n",
    "    \"\"\"\n",
    "    文本词表：词元到索引的映射\n",
    "    \n",
    "    主要功能：\n",
    "    - 统计词频，按频率排序\n",
    "    - 创建词元到索引的映射\n",
    "    - 支持未知词元处理\n",
    "    - 提供索引到词元的反向映射\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):  # 初始化方法，参数：词元列表，最小频率，保留词元\n",
    "        \"\"\"\n",
    "        初始化词汇表\n",
    "        \n",
    "        参数：\n",
    "            tokens: 词元列表，可以是1D或2D列表\n",
    "            min_freq: 最小词频阈值，低于此频率的词元将被过滤\n",
    "            reserved_tokens: 保留词元列表，如特殊符号<pad>、<bos>等\n",
    "        \n",
    "        处理流程：\n",
    "        1. 统计所有词元的频率\n",
    "        2. 按频率降序排序\n",
    "        3. 过滤低频词元\n",
    "        4. 创建索引映射（未知词元索引为0）\n",
    "        \"\"\"\n",
    "        if tokens is None:  # 如果tokens为None\n",
    "            tokens = []  # 设置为空列表\n",
    "        if reserved_tokens is None:  # 如果reserved_tokens为None\n",
    "            reserved_tokens = []  # 设置为空列表\n",
    "            \n",
    "        # 按出现频率排序词元\n",
    "        counter = count_corpus(tokens)  # 统计词频\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],  # 按频率降序排序\n",
    "                                   reverse=True)\n",
    "        \n",
    "        # 未知词元的索引为0\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens  # 设置未知词元索引，初始化唯一词元列表\n",
    "        \n",
    "        # 添加满足频率要求的词元\n",
    "        uniq_tokens += [  # 扩展唯一词元列表\n",
    "            token for token, freq in self._token_freqs  # 遍历词频对\n",
    "            if freq >= min_freq and token not in uniq_tokens  # 如果频率足够且不在列表中\n",
    "        ]\n",
    "        \n",
    "        # 创建双向映射\n",
    "        self.index_to_token, self.token_to_idx = [], dict()  # 初始化索引到词元和词元到索引的映射\n",
    "        for token in uniq_tokens:  # 遍历唯一词元\n",
    "            self.index_to_token.append(token)  # 添加到索引到词元列表\n",
    "            self.token_to_idx[token] = len(self.index_to_token) - 1  # 设置词元到索引映射\n",
    "\n",
    "    def __len__(self):  # 定义长度方法\n",
    "        \"\"\"返回词汇表大小\"\"\"\n",
    "        return len(self.index_to_token)  # 返回索引到词元列表的长度\n",
    "\n",
    "    def __getitem__(self, tokens):  # 定义索引方法，支持词元到索引转换\n",
    "        \"\"\"\n",
    "        将词元转换为索引\n",
    "        \n",
    "        参数：\n",
    "            tokens: 单个词元或词元列表\n",
    "        \n",
    "        返回：\n",
    "            对应的索引或索引列表\n",
    "        \"\"\"\n",
    "        if not isinstance(tokens, (list, tuple)):  # 如果不是列表或元组（单个词元）\n",
    "            return self.token_to_idx.get(tokens, self.unk)  # 返回索引，未知词元返回unk索引\n",
    "        return [self.__getitem__(token) for token in tokens]  # 递归转换列表中的每个词元\n",
    "\n",
    "    def to_tokens(self, indices):  # 定义反向转换方法\n",
    "        \"\"\"\n",
    "        将索引转换为词元\n",
    "        \n",
    "        参数：\n",
    "            indices: 单个索引或索引列表\n",
    "        \n",
    "        返回：\n",
    "            对应的词元或词元列表\n",
    "        \"\"\"\n",
    "        if not isinstance(indices, (list, tuple)):  # 如果不是列表或元组（单个索引）\n",
    "            return self.index_to_token[indices]  # 返回对应的词元\n",
    "        return [self.index_to_token[index] for index in indices]  # 返回索引列表对应的词元列表\n",
    "\n",
    "def count_corpus(tokens):  #@save  # 定义词频统计函数\n",
    "    \"\"\"\n",
    "    统计词元的频率\n",
    "    \n",
    "    参数：\n",
    "        tokens: 词元列表，可以是1D或2D列表\n",
    "    \n",
    "    返回：\n",
    "        词元频率的Counter对象\n",
    "    \n",
    "    处理逻辑：\n",
    "    - 如果输入是2D列表（多行文本），先展平为一维\n",
    "    - 使用collections.Counter统计每个词元的出现次数\n",
    "    \"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):  # 如果tokens为空或第一个元素是列表（2D）\n",
    "        tokens = [token for line in tokens for token in line]  # 展平为1D列表\n",
    "    return collections.Counter(tokens)  # 返回词频统计结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f000f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "# ==================== 创建词汇表实例 ====================\n",
    "# 使用分词后的词元创建词汇表\n",
    "# 这将建立词元到数字索引的映射关系\n",
    "vocab = Vocab(tokens)  # 使用分词结果创建词汇表实例\n",
    "\n",
    "# 显示词汇表前10个词元及其索引\n",
    "# 观察：\n",
    "# - '<unk>': 索引0，未知词元\n",
    "# - 最常见的词元有较小的索引\n",
    "print(list(vocab.token_to_idx.items())[:10])  # 打印词汇表前10项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba31ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "索引: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "文本: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "# ==================== 文本到索引的转换示例 ====================\n",
    "# 演示如何将原始文本转换为数字索引序列\n",
    "# 这展示了NLP预处理的完整流程\n",
    "\n",
    "for i in [0, 10]:  # 遍历第0行和第10行\n",
    "    print('文本:', tokens[i])  # 打印原始文本\n",
    "    \n",
    "    print('索引:', vocab[tokens[i]])  # 打印对应的索引序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d03c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 词元数: 170580\n",
      "# 词表大小: 28\n"
     ]
    }
   ],
   "source": [
    "# ==================== 加载时光机器语料库 ====================\n",
    "# 什么是语料库？\n",
    "# - 经过预处理的文本数据集\n",
    "# - 通常是词元索引的序列\n",
    "# - 用于训练语言模型或进行其他NLP任务\n",
    "\n",
    "def load_corpus_time_machine(max_tokens=-1):  # 定义加载语料库函数，参数：最大词元数\n",
    "    \"\"\"\n",
    "    返回时光机器数据集的词元索引列表和词表\n",
    "    \n",
    "    参数：\n",
    "        max_tokens: 最大词元数量，-1表示使用全部词元\n",
    "    \n",
    "    返回：\n",
    "        corpus: 词元索引的列表（一维）\n",
    "        vocab: 词汇表对象\n",
    "    \n",
    "    处理流程：\n",
    "    1. 读取原始文本行\n",
    "    2. 按字符分词（而不是单词）\n",
    "    3. 创建词汇表\n",
    "    4. 将所有词元转换为索引\n",
    "    5. 可选：截取前max_tokens个词元\n",
    "    \"\"\"\n",
    "    lines = read_time_machine()  # 读取文本行\n",
    "    tokens = tokenize(lines, 'char')  # 按字符分词\n",
    "    vocab = Vocab(tokens)  # 创建词汇表\n",
    "    \n",
    "    # 将文本转换为词元的索引表示\n",
    "    corpus = [vocab[token] for line in tokens for token in line]  # 展平并转换为索引\n",
    "    \n",
    "    if max_tokens > 0:  # 如果指定了最大词元数\n",
    "        corpus = corpus[:max_tokens]  # 截取前max_tokens个词元\n",
    "    \n",
    "    return corpus, vocab  # 返回语料库和词汇表\n",
    "\n",
    "# 加载完整的语料库\n",
    "corpus, vocab = load_corpus_time_machine()  # 调用函数加载语料库\n",
    "\n",
    "# 输出语料库的基本信息\n",
    "print('# 词元数:', len(corpus))  # 打印词元总数\n",
    "print('# 词表大小:', len(vocab))  # 打印词汇表大小"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
